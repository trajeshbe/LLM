{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
      },
      "source": [
        "### Necessary imports"
      ],
      "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "#!pip install -q accelerate==0.21.0 peft==0.5.0 bitsandbytes==0.40.2 trl==0.4.7"
      ],
      "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers datasets trl peft accelerate bitsandbytes auto-gptq optimum\n"
      ],
      "metadata": {
        "id": "Bk5Ul5QnA5zJ"
      },
      "id": "Bk5Ul5QnA5zJ",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets"
      ],
      "metadata": {
        "id": "KeAnwf5o-PZJ"
      },
      "id": "KeAnwf5o-PZJ",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
      },
      "source": [
        "### Dependencies"
      ],
      "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain"
      ],
      "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "3hiV4sq1Ah6W"
      },
      "id": "3hiV4sq1Ah6W",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install auto-gptq"
      ],
      "metadata": {
        "id": "5wG4hI1jHzc8"
      },
      "id": "5wG4hI1jHzc8",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install optimum"
      ],
      "metadata": {
        "id": "0Z5EXOcsH_NJ"
      },
      "id": "0Z5EXOcsH_NJ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
      },
      "source": [
        "### Load quantized Zephyr 7B"
      ],
      "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80f94e97-7e58-4253-9376-73af6f36e139",
        "outputId": "5a48cf79-afcf-41ce-effd-105ec2659544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TheBloke/zephyr-7B-beta-GPTQ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "#Note you can try to replce the Model of your Choice and it would mostly\n",
        "#work with any Quantized Models\n",
        "#model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "model_name='TheBloke/zephyr-7B-beta-GPTQ'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "bnb_config = GPTQConfig(bits=4,\n",
        "                        disable_exllama=True,\n",
        "                        device_map=\"auto\",\n",
        "                        use_cache=False,\n",
        "                        lora_r=16,\n",
        "                        lora_alpha=16,\n",
        "                        tokenizer=tokenizer\n",
        "                                )\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "#model_name_or_path = \"TheBloke/notus-7B-v1-GPTQ\"\n",
        "print(model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "#model = AutoModelForCausalLM.from_pretrained(\n",
        "#    model_name,\n",
        "#    quantization_config=bnb_config,\n",
        "#)"
      ],
      "id": "80f94e97-7e58-4253-9376-73af6f36e139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
      },
      "source": [
        "### Count number of trainable parameters"
      ],
      "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "91d2a86e-69e8-496f-b388-853168537c20",
        "outputId": "81693202-7c54-4255-f56c-5e27e3385307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable model parameters: 262410240\n",
            "all model parameters: 262410240\n",
            "percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "id": "91d2a86e-69e8-496f-b388-853168537c20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
      },
      "source": [
        "### Build Zephyr text generation pipeline"
      ],
      "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=200,\n",
        ")"
      ],
      "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
      },
      "outputs": [],
      "source": [
        "Zephyr_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
      },
      "source": [
        "### Load and chunk documents. Load chunked documents into FAISS index"
      ],
      "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
      },
      "outputs": [],
      "source": [],
      "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHtgKDFO7xrD"
      },
      "id": "UHtgKDFO7xrD",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import AmazonTextractPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "_ddiCSlf7fwT"
      },
      "id": "_ddiCSlf7fwT",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "#embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "#loader_story_part1 = CSVLoader(\"/content/sample_data/simple_story_question_answers_for_finetuning.csv\")\n",
        "#loader_story_part2 = CSVLoader(\"/content/sample_data/simple_extended_story_question_answers_for_rag.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "91AsUCQE8ArD"
      },
      "id": "91AsUCQE8ArD",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "import os , time\n",
        "pwd=os.getcwd()\n",
        "st=time.time()\n",
        "\n",
        "loader=DirectoryLoader(path=pwd,glob=\"**/simple*.csv\",use_multithreading=True, loader_cls=TextLoader)\n",
        "documents=loader.load()\n",
        "endt=time.time()\n",
        "et=endt-st\n",
        "print(et)\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiXWgvbVdbS-",
        "outputId": "2ac9a8b7-0055-403d-8cc7-11eb77ea773c"
      },
      "id": "MiXWgvbVdbS-",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0029408931732177734\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USjdqN32Zful"
      },
      "id": "USjdqN32Zful",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMIz2-JGcQXh"
      },
      "id": "qMIz2-JGcQXh",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23BOwAcL9cT7"
      },
      "id": "23BOwAcL9cT7",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
      },
      "outputs": [],
      "source": [],
      "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk text to keep it meaningful size\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "\n",
        "all_chunks = []\n",
        "\n",
        "document = loader.load()\n",
        "chunks = splitter.split_documents(document)\n",
        "all_chunks += chunks"
      ],
      "metadata": {
        "id": "-7rVsIrLBKOF"
      },
      "id": "-7rVsIrLBKOF",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an Embedding model for vector representation of text\n",
        "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "LOJZogjaBgNK"
      },
      "id": "LOJZogjaBgNK",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed chunks\n",
        "#FAISS is Facebook/Meta's vector embedding database\n",
        "db = FAISS.from_documents(all_chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "ltWlfs8LBh34"
      },
      "id": "ltWlfs8LBh34",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "1Xl5vqu5dVQl"
      },
      "id": "1Xl5vqu5dVQl",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": true,
        "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
      },
      "outputs": [],
      "source": [],
      "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
      },
      "source": [
        "### Create PromptTemplate and LLMChain"
      ],
      "id": "93d54a0b-bf6c-4a24-b888-4c3283b9ccf6"
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
      },
      "outputs": [],
      "source": [
        "##### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your  knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=Zephyr_llm, prompt=prompt)"
      ],
      "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
      },
      "source": [
        "### Build RAG Chain"
      ],
      "id": "c1e75d34-cf63-49a8-a671-88ccb5444367"
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "a1e18178-46f2-4b87-86c4-d13ff5219968",
        "outputId": "18890faa-a84f-4b40-dff4-f88cddf643a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# Ask Question 1\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Where did Vendhan and Maria organize the procession inspired by Panguni Uthiram?\")"
      ],
      "id": "a1e18178-46f2-4b87-86c4-d13ff5219968"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0",
        "outputId": "87ff12c5-274b-402b-e7f7-47051f20bb29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Where did Vendhan and Maria organize the procession inspired by Panguni Uthiram?,Lisbon', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='Where did Vendhan and Maria organize the procession inspired by Panguni Uthiram?,Lisbon', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='Where did Vendhan and Maria organize the procession inspired by Panguni Uthiram?,Lisbon', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='Where did Vendhan and Maria organize the procession inspired by Panguni Uthiram?,Lisbon', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "# Retrives the relavant data from the questionaire dataset loaded into the Vector DB FAISS\n",
        "# It takes our input question (Query), converts into a sentence embedding vector and searches\n",
        "#for the closet possible vectors in the DB which are similar to the question asked.\n",
        "result['context']"
      ],
      "id": "ef54e9e3-4fa5-4676-baaf-8b11e3f09dc0"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "62895f39-9dfb-4f58-8312-72580ca03a20",
        "outputId": "819eb183-af2b-4146-8f03-46b41f4c4b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "According to the given context, Vendhan and Maria organized the procession inspired by Panguni Uthiram in Lisbon. This information can be found in four separate documents with identical content.\n"
          ]
        }
      ],
      "source": [
        "# Answer to the Queston 1\n",
        "print(result['text'])"
      ],
      "id": "62895f39-9dfb-4f58-8312-72580ca03a20"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rbcFjWreIXb"
      },
      "id": "5rbcFjWreIXb",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEY7BjJWJ-zU"
      },
      "id": "AEY7BjJWJ-zU",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "63253cbc-9de3-41b9-a5c8-b672333f479c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f141e1c9-6490-45ee-c996-c362917ce4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What was the universal language that Vendhan and Maria discovered?\")"
      ],
      "id": "63253cbc-9de3-41b9-a5c8-b672333f479c"
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgYN6-WpJ_jT",
        "outputId": "b230bbe5-0fb5-488b-f38b-c26c39f28372"
      },
      "id": "zgYN6-WpJ_jT",
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What was the universal language that Vendhan and Maria discovered?,Love and compassion', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the universal language that Vendhan and Maria discovered?,Love and compassion', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the universal language that Vendhan and Maria discovered?,Love and compassion', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the universal language that Vendhan and Maria discovered?,Love and compassion', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am6_xLZteTOR",
        "outputId": "70fa80a5-a4e3-4999-fa27-3fb3cbcf379d"
      },
      "id": "am6_xLZteTOR",
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Answer: Love and compassion were the universal language that Vendhan and Maria discovered, as stated in the given context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKy6tO9rbMAw"
      },
      "id": "qKy6tO9rbMAw",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 3\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Who is Aadhan?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsSufdJ1bMD2",
        "outputId": "9388c49b-69a5-4959-c688-9d7e87a12bea"
      },
      "id": "xsSufdJ1bMD2",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22mPZpoubXYU",
        "outputId": "96f1e27c-04c5-4b7d-eacc-d74a8fe4e9f9"
      },
      "id": "22mPZpoubXYU",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='who is Aadhan?,The Emperror of Kandigai\\nWho is Aadhan?,Aadhan is a noble king.', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='Who faced assassination attempts?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='Who encountered challenges in solidifying his rule?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='Who faced challenges in reclaiming his destiny?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt9wPcKobZsW",
        "outputId": "28039b8b-d9c6-44f4-9227-f0b2e9d5ad21"
      },
      "id": "Vt9wPcKobZsW",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### ANSWER:\n",
            "Aadhan is a noble king, as mentioned in the given context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Who is Aadhan?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS-2yHjyffkT",
        "outputId": "e098c7aa-f574-4746-8f42-a6f885586c4a"
      },
      "id": "NS-2yHjyffkT",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2jbqZYQfmJn",
        "outputId": "00596438-6eaf-4bfe-a853-e06e810552df"
      },
      "id": "C2jbqZYQfmJn",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='who is Aadhan?,The Emperror of Kandigai\\nWho is Aadhan?,Aadhan is a noble king.', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='Who faced assassination attempts?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='Who encountered challenges in solidifying his rule?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='Who faced challenges in reclaiming his destiny?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbAXHTS-hCx1",
        "outputId": "769585dc-c01b-4ab4-e1a7-04efa85d8783"
      },
      "id": "UbAXHTS-hCx1",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### ANSWER:\n",
            "Aadhan is a noble king, as mentioned in the given context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What would be your advice for Amudhan?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R1fJgfYDjYy",
        "outputId": "3045c647-1661-49bd-d16c-4d9ae6015ee0"
      },
      "id": "8R1fJgfYDjYy",
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rDuhPGCDnFT",
        "outputId": "86871b99-99ba-4350-fc96-769c453e4d13"
      },
      "id": "6rDuhPGCDnFT",
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What did Amudhan do after Aadhan left?,Ascended to the throne', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='What kind of ruler was Amudhan?,Tyrant', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content=\"Who faced hardship?,Citizens under Amudhan's rule\", metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'}),\n",
              " Document(page_content='Who faced challenges in confronting Amudhan?,Aadhan', metadata={'source': '/content/sample_data/simple_story_question_answers_for_finetuning.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDAB6ZzSEVUJ"
      },
      "id": "VDAB6ZzSEVUJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p61lGcwYDouX",
        "outputId": "9ccd5eb4-429b-498c-a84e-53e02549a02f"
      },
      "id": "p61lGcwYDouX",
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Based on the given context, it seems that Amudhan's actions as a ruler have led to hardships for his citizens. My advice for him would be to reevaluate his leadership style and prioritize the needs and well-being of his people over his own desires. As a tyrant, he may need to learn to govern with fairness, justice, and compassion. It is also clear from the text that Aadhan faced challenges in confronting Amudhan. Therefore, I suggest that Amudhan should consider listening to Aadhan's perspective and working together towards a more just and equitable society. Ultimately, Amudhan must recognize that true power lies not in oppression but in serving the interests of his people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFI85pAbEWtc"
      },
      "id": "WFI85pAbEWtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What was the situation when Vedhan and Maria met for the first time?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R9lXMriEXAt",
        "outputId": "4004c6cd-8261-43c7-a2e5-4688ca6fc498"
      },
      "id": "0R9lXMriEXAt",
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNjnvYQgEuzT",
        "outputId": "e899a23e-bc7e-458b-87d8-7c1bfc55d381"
      },
      "id": "FNjnvYQgEuzT",
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What did Vendhan and Maria exchange throughout their travels?,Cultural traditions and experiences', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What did Vendhan and Maria exchange throughout their travels?,Cultural traditions and experiences', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What did Vendhan and Maria exchange throughout their travels?,Cultural traditions and experiences', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What did Vendhan and Maria exchange throughout their travels?,Cultural traditions and experiences', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQb44K9aFB1e",
        "outputId": "033f24bc-e472-4e5f-ced6-345c5c44db69"
      },
      "id": "SQb44K9aFB1e",
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "I do not have access to the specific story you are referring to involving vedhan and maria. Please provide more context or specify which story you are asking about so I can accurately answer your question. Based on the given question, it seems like you are asking what cultural traditions and experiences vedhan and maria exchanged during their travels. However, without knowing the context of their meeting, it is unclear if this happened at their first encounter. Therefore, I suggest providing more information to clarify the question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets expand the Context to use custom inputs via prompts"
      ],
      "metadata": {
        "id": "GmJiPsKzFbTh"
      },
      "id": "GmJiPsKzFbTh",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n"
      ],
      "metadata": {
        "id": "XZ8EX-55HmqH"
      },
      "id": "XZ8EX-55HmqH",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = FAISS.from_texts(\n",
        "    [\"As time passed by Aadhan and Thamarai's son Vendhan, a prince turned 20. He was wise and disciplined wanted to explore the world. In the mean time, a trader from Portugal Cristiano Ronaldo visited the hills of Kandigai in search of some spices like Cardamom and pepper. He was with his family i.e his wife Carolina and beautiful daughter Maria. They were trekking on the hills. The Prince Vendhan was on the hills too in his horse to visit his friend Cheran who lives on top of the hill. He heard a cry for help and rushed towards the voice. He noticed Maria running and being chased by a bear. The prince rushed in this horse and picked Maria and rescued her. Saving her life, there emerged a romance. They soon fell in love. Maria took Vendhan to Portugal for a tour and explored the Portuguese culture\"], embedding=embeddings\n",
        ")\n",
        "retriever1 = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "rV0jwydmQMnW"
      },
      "id": "rV0jwydmQMnW",
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNMDyV29Hav9"
      },
      "id": "ZNMDyV29Hav9",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGYJdO8UWAEg"
      },
      "id": "iGYJdO8UWAEg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        " {\"context\": retriever1, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "-_5I1zNDJzOp"
      },
      "id": "-_5I1zNDJzOp",
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvNhEFVc9rGm"
      },
      "id": "mvNhEFVc9rGm",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=rag_chain.invoke(\"Where did Vendhan and Maria meet?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTd5CFCALYf2",
        "outputId": "8773cecc-e791-4378-9aa1-3c23b063daaf"
      },
      "id": "WTd5CFCALYf2",
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9bst3AC_w6_",
        "outputId": "e3039a09-c878-4574-cb54-a5d1f3c8267f"
      },
      "id": "p9bst3AC_w6_",
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"As time passed by Aadhan and Thamarai's son Vendhan, a prince turned 20. He was wise and disciplined wanted to explore the world. In the mean time, a trader from Portugal Cristiano Ronaldo visited the hills of Kandigai in search of some spices like Cardamom and pepper. He was with his family i.e his wife Carolina and beautiful daughter Maria. They were trekking on the hills. The Prince Vendhan was on the hills too in his horse to visit his friend Cheran who lives on top of the hill. He heard a cry for help and rushed towards the voice. He noticed Maria running and being chased by a bear. The prince rushed in this horse and picked Maria and rescued her. Saving her life, there emerged a romance. They soon fell in love. Maria took Vendhan to Portugal for a tour and explored the Portuguese culture\")]"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA8RkOSNPNQm",
        "outputId": "2fe84e40-9d6e-4bc2-a178-1abbd49d3648"
      },
      "id": "pA8RkOSNPNQm",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Vendhan and Maria met while Vendhan was rescuing Maria from being chased by a bear in the hills of Kandigai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xf3OnQsvlJAH"
      },
      "id": "Xf3OnQsvlJAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7\n",
        "rag_chain = (\n",
        " {\"context\": retriever1, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What was the situation when Vedhan and Maria met for the first time?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHqDXUoNk7Ss",
        "outputId": "d020982d-2d2c-4fdd-a202-ce760b561739"
      },
      "id": "QHqDXUoNk7Ss",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq1FpjNYlUuZ",
        "outputId": "e2945f1a-0e55-4339-ee00-66e5bc7fee8a"
      },
      "id": "Wq1FpjNYlUuZ",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Vedhan and Maria did not meet for the first time in the given context. The text describes how Vedhan saved Maria from a bear while she was being chased by it during her trekking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What happened in Lisbon?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvInzM-BpWdm",
        "outputId": "5892b59c-2466-4927-88b5-597d609179c2"
      },
      "id": "cvInzM-BpWdm",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahGVe5wCq7Z-",
        "outputId": "08d8c7d1-c823-4f32-c6b3-5d0c0ae0b6cc"
      },
      "id": "ahGVe5wCq7Z-",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What was the resounding success of the event in Lisbon?,Bringing together people from all walks of', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the resounding success of the event in Lisbon?,Bringing together people from all walks of', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the resounding success of the event in Lisbon?,Bringing together people from all walks of', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What was the resounding success of the event in Lisbon?,Bringing together people from all walks of', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPg4EQKXpqMq",
        "outputId": "29ef9165-9e22-45af-9845-20778dd4c286"
      },
      "id": "IPg4EQKXpqMq",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Based on the given context, it can be inferred that a successful event took place in Lisbon and it brought together people from diverse backgrounds or walks of life. However, without further information provided, it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"What is Panguni Uthiram?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzkmAs-9r1ai",
        "outputId": "f39f1c47-6e22-48e9-dba2-0492748d6e2b"
      },
      "id": "kzkmAs-9r1ai",
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTlNbVLhr-zH",
        "outputId": "c52d67a5-88c7-42d9-aaad-146cf8d9b4ae"
      },
      "id": "iTlNbVLhr-zH",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='What is Panguni Uthiram celebrated for?,Divine union of Lord Shiva and Goddess Parvati', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What is Panguni Uthiram celebrated for?,Divine union of Lord Shiva and Goddess Parvati', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What is Panguni Uthiram celebrated for?,Divine union of Lord Shiva and Goddess Parvati', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'}),\n",
              " Document(page_content='What is Panguni Uthiram celebrated for?,Divine union of Lord Shiva and Goddess Parvati', metadata={'source': '/content/sample_data/simple_extended_story_question_answers_for_rag.csv'})]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlDrvwE5sEY3",
        "outputId": "5ab4a060-09ff-4e7f-fc9f-76832c3a7c90"
      },
      "id": "UlDrvwE5sEY3",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "Panguni Uthiram is a celebration that commemorates the divine union between Lord Shiva and Goddess Parvati. Based on the information provided in the given context, it seems that this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"Describe some features of celebration during Panguni Uthiram?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc54QLOMs0ck",
        "outputId": "231c7fcd-f7b5-4717-cf53-19bcea952a53"
      },
      "id": "oc54QLOMs0ck",
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0MudiDxs-Zn",
        "outputId": "42fa61e0-fa2f-47cb-ae94-cda60b602f05"
      },
      "id": "i0MudiDxs-Zn",
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "During the Panguni Uthiram festival, elaborate processions and traditional dances are some of its prominent features. These celebrations are a part of the Tamil Hindu calendar and typically take place in March or April each year. The processions involve decorated floats carrying deities through the streets, accompanied by devotees singing hymns and chanting prayers. Traditional dance forms such as Bharatanatyam, Kavadi Attam, and Thiruvathira Kali are also performed during this festival, adding to the cultural richness of the celebrations. Overall, the Panguni Uthiram festival is a vibrant and colorful affair that showcases the deep-rooted traditions and beliefs of the Tamil community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n7NYUz1Uwinb"
      },
      "id": "n7NYUz1Uwinb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9\n",
        "# We are exploring the capabilities of LLM to answer questions beyound\n",
        "# the context as well.\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\" What is kavadi Attam? Please answer even if it's not part of context\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AknuH4AbwOhk",
        "outputId": "c6054e96-f604-468a-996d-7a80f03a7f63"
      },
      "id": "AknuH4AbwOhk",
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ua9km_wjh2",
        "outputId": "a3a2648d-54aa-4072-fc35-2be390ee20a4"
      },
      "id": "16ua9km_wjh2",
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|assistant|>\n",
            "I do not have prior knowledge or context outside of what is provided in the given text. However, based on my research, \"kavadi attam\" refers to a traditional tamil festival that involves carrying a heavy structure called a kavadi as an act of penance and devotion to god murugan. It is typically performed during the thaipusam festival, which falls during the tamil month of thai (january-february).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "UjzoJj8wEczF"
      },
      "id": "UjzoJj8wEczF",
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > colab_package_dependencies_requirements.txt"
      ],
      "metadata": {
        "id": "R6p1Wb_RDvHr"
      },
      "id": "R6p1Wb_RDvHr",
      "execution_count": 193,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-gpu.m114",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}