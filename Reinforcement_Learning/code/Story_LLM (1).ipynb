{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNsu0NefMSvgAnoJoCTLyBC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0Ot8TL2pJsuY","executionInfo":{"status":"error","timestamp":1741328486640,"user_tz":-330,"elapsed":714490,"user":{"displayName":"Rajesh Learning","userId":"02323984841585441667"}},"outputId":"bbc61f27-fa38-491a-b3d2-79815955267e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Collecting litgpt[all]\n","  Downloading litgpt-0.5.7-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch<2.6.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (2.5.1+cu124)\n","Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (1.26.4)\n","Collecting lightning<2.6.0,>=2.5.0 (from litgpt[all])\n","  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonargparse<=4.32.1,>=4.30.1 (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt[all])\n","  Downloading jsonargparse-4.32.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: huggingface_hub>=0.23.5 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (0.28.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (0.5.3)\n","Requirement already satisfied: tokenizers>=0.15.2 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (0.21.0)\n","Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (4.67.1)\n","Collecting lightning-thunder>=0.2.0.dev20250119 (from litgpt[all])\n","  Downloading lightning_thunder-0.2.2.dev20250302-py3-none-any.whl.metadata (13 kB)\n","Collecting bitsandbytes<0.44.2,>=0.44.0 (from litgpt[all])\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (0.2.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (2.32.3)\n","Collecting litdata==0.2.17 (from litgpt[all])\n","  Downloading litdata-0.2.17-py3-none-any.whl.metadata (31 kB)\n","Collecting litserve<=0.2.4 (from litgpt[all])\n","  Downloading litserve-0.2.4-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: zstandard>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (0.23.0)\n","Requirement already satisfied: pandas>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (2.2.2)\n","Requirement already satisfied: pyarrow>=15.0.2 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (18.1.0)\n","Requirement already satisfied: tensorboard>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from litgpt[all]) (2.18.0)\n","Collecting torchmetrics>=1.3.1 (from litgpt[all])\n","  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n","Collecting datasets>=2.18.0 (from litgpt[all])\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Collecting transformers==4.47.1 (from litgpt[all])\n","  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lm-eval>=0.4.2 (from litgpt[all])\n","  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvloop>=0.2.0 (from litgpt[all])\n","  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from litdata==0.2.17->litgpt[all]) (3.17.0)\n","Collecting boto3 (from litdata==0.2.17->litgpt[all])\n","  Downloading boto3-1.37.8-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1->litgpt[all]) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1->litgpt[all]) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1->litgpt[all]) (2024.11.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.18.0->litgpt[all])\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets>=2.18.0->litgpt[all])\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.18.0->litgpt[all])\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.18.0->litgpt[all]) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->litgpt[all]) (3.11.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.5->litgpt[all]) (4.12.2)\n","Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]>=0.21.0; extra == \"all\"->litgpt[all])\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt[all]) (0.16)\n","Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt[all])\n","  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading lightning_utilities-0.13.1-py3-none-any.whl.metadata (5.6 kB)\n","Collecting pytorch-lightning (from lightning<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n","Collecting looseversion==1.3.0 (from lightning-thunder>=0.2.0.dev20250119->litgpt[all])\n","  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: networkx>=3.3 in /usr/local/lib/python3.11/dist-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt[all]) (3.4.2)\n","Requirement already satisfied: optree>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt[all]) (0.14.1)\n","Requirement already satisfied: opt_einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt[all]) (3.4.0)\n","Requirement already satisfied: mpmath<1.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt[all]) (1.3.0)\n","Collecting fastapi>=0.100 (from litserve<=0.2.4->litgpt[all])\n","  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from litserve<=0.2.4->litgpt[all]) (0.28.1)\n","Collecting uvicorn>=0.29.0 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all])\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.2->litgpt[all]) (1.3.0)\n","Collecting evaluate (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting jsonlines (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.2->litgpt[all]) (2.10.2)\n","Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.2->litgpt[all]) (0.14.0)\n","Collecting pybind11>=2.6.2 (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Collecting pytablewriter (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n","Collecting rouge-score>=0.0.4 (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sacrebleu>=1.5.0 (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.2->litgpt[all]) (1.6.1)\n","Collecting sqlitedict (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm-multiprocess (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n","Collecting word2number (from lm-eval>=0.4.2->litgpt[all])\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.2->litgpt[all]) (10.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.9.0->litgpt[all]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.9.0->litgpt[all]) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.9.0->litgpt[all]) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->litgpt[all]) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->litgpt[all]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->litgpt[all]) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->litgpt[all]) (2025.1.31)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (1.70.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (4.25.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.0->litgpt[all]) (3.1.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.5.0->litgpt[all]) (3.1.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.5.0->litgpt[all]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.5.0->litgpt[all]) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.6.0,>=2.5.0->litgpt[all])\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.5.0->litgpt[all]) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.6.0,>=2.5.0->litgpt[all]) (1.13.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval>=0.4.2->litgpt[all]) (5.9.5)\n","Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.100->litserve<=0.2.4->litgpt[all])\n","  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.100->litserve<=0.2.4->litgpt[all]) (2.10.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->litgpt[all]) (1.18.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm-eval>=0.4.2->litgpt[all]) (3.9.1)\n","Collecting portalocker (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all])\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]) (0.9.0)\n","Collecting colorama (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all])\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]) (5.3.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]) (3.5.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt[all]) (6.5.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.29.0->uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.29.0->uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]) (0.14.0)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all])\n","  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all])\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all])\n","  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]) (14.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.14.0->litgpt[all]) (3.0.2)\n","Collecting botocore<1.38.0,>=1.37.8 (from boto3->litdata==0.2.17->litgpt[all])\n","  Downloading botocore-1.37.8-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->litdata==0.2.17->litgpt[all])\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->litdata==0.2.17->litgpt[all])\n","  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->litserve<=0.2.4->litgpt[all]) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->litserve<=0.2.4->litgpt[all]) (1.0.7)\n","Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n","Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n","Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n","Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n","Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval>=0.4.2->litgpt[all])\n","  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval>=0.4.2->litgpt[all]) (5.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100->litserve<=0.2.4->litgpt[all]) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100->litserve<=0.2.4->litgpt[all]) (2.27.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->litserve<=0.2.4->litgpt[all]) (1.3.1)\n","Downloading litdata-0.2.17-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonargparse-4.32.1-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_thunder-0.2.2.dev20250302-py3-none-any.whl (866 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.8/866.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n","Downloading litserve-0.2.4-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading litgpt-0.5.7-py3-none-any.whl (176 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.6/176.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.13.1-py3-none-any.whl (28 kB)\n","Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.37.8-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.37.8-py3-none-any.whl (13.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n","Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n","Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.46.0-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n","Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n","Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n","Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Building wheels for collected packages: rouge-score, sqlitedict, word2number\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a0ef7d512fbd15b9e9ea6e2ad6d7df4b66429e86317a536afbc36393abfa0a68\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=1bc12125cd6fca56f3610352e6a33fa89e931e3982a18f6e8869a68e124c9ae7\n","  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=2f9b4da7f146453ae5cf7bb5ae4629161c05d93db8c131f37114a54ad08da92f\n","  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n","Successfully built rouge-score sqlitedict word2number\n","Installing collected packages: word2number, sqlitedict, looseversion, xxhash, uvloop, uvicorn, typeshed-client, tcolorpy, python-dotenv, pybind11, portalocker, pathvalidate, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mbstrdecoder, lightning-utilities, jsonlines, jsonargparse, jmespath, httptools, hf-transfer, dill, colorama, watchfiles, typepy, tqdm-multiprocess, starlette, sacrebleu, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, botocore, s3transfer, nvidia-cusolver-cu12, fastapi, transformers, litserve, datasets, DataProperty, boto3, torchmetrics, tabledata, litdata, lightning-thunder, evaluate, bitsandbytes, pytorch-lightning, pytablewriter, lm-eval, lightning, litgpt\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.48.3\n","    Uninstalling transformers-4.48.3:\n","      Successfully uninstalled transformers-4.48.3\n","Successfully installed DataProperty-1.1.0 bitsandbytes-0.44.1 boto3-1.37.8 botocore-1.37.8 colorama-0.4.6 datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 fastapi-0.115.11 hf-transfer-0.1.9 httptools-0.6.4 jmespath-1.0.1 jsonargparse-4.32.1 jsonlines-4.0.0 lightning-2.5.0.post0 lightning-thunder-0.2.2.dev20250302 lightning-utilities-0.13.1 litdata-0.2.17 litgpt-0.5.7 litserve-0.2.4 lm-eval-0.4.8 looseversion-1.3.0 mbstrdecoder-1.1.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathvalidate-3.2.3 portalocker-3.1.1 pybind11-2.13.6 pytablewriter-1.2.1 python-dotenv-1.0.1 pytorch-lightning-2.5.0.post0 rouge-score-0.1.2 s3transfer-0.11.4 sacrebleu-2.5.1 sqlitedict-2.1.0 starlette-0.46.0 tabledata-1.3.4 tcolorpy-0.1.7 torchmetrics-1.6.2 tqdm-multiprocess-0.0.11 transformers-4.47.1 typepy-1.3.4 typeshed-client-2.7.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 word2number-1.1 xxhash-3.5.0\n","torch version: 2.5.1+cu124\n","tiktoken version: 0.9.0\n","Total number of character: 9728\n","Once Aadhan renounced his kingdom for the love of Thamarai, they embarked on a journey far from the\n","['Once', ' ', 'Aadhan', ' ', 'renounced', ' ', 'his', ' ', 'kingdom', ' ', 'for', ' ', 'the', ' ', 'love', ' ', 'of', ' ', 'Thamarai', ',', ' ', 'they', ' ', 'embarked', ' ', 'on', ' ', 'a', ' ', 'journey', ' ', 'far', ' ', 'from', ' ', 'the', ' ', 'political']\n","Number of tokens: 3290\n","616\n","('\\n', 0)\n","(' ', 1)\n","(\"'\", 2)\n","(',', 3)\n","('.', 4)\n","('23rd', 5)\n","('24th', 6)\n","('Aadhan', 7)\n","('After', 8)\n","('Algarve', 9)\n","('Amudhan', 10)\n","('And', 11)\n","('As', 12)\n","('Despite', 13)\n","('Determined', 14)\n","('During', 15)\n","('Feast', 16)\n","('From', 17)\n","('Gathering', 18)\n","('Goddess', 19)\n","('However', 20)\n","('In', 21)\n","('Inspired', 22)\n","('John', 23)\n","('João', 24)\n","('June', 25)\n","('Kandigai', 26)\n","('Lisbon', 27)\n","('Lord', 28)\n","('Madeira', 29)\n","('Maria', 30)\n","('Meanwhile', 31)\n","('Nadu', 32)\n","('On', 33)\n","('Once', 34)\n","('One', 35)\n","('Over', 36)\n","('Panguni', 37)\n","('Parvati', 38)\n","('Pongal', 39)\n","('Porto', 40)\n","('Portugal', 41)\n","('Portuguese', 42)\n","('She', 43)\n","('Shiva', 44)\n","('St', 45)\n","('São', 46)\n","('Tamil', 47)\n","('Thamarai', 48)\n","('The', 49)\n","('Their', 50)\n","[59, 48, 138, 304, 496, 3, 7, 335, 107, 553, 304, 307, 4]\n","tiktoken version: 0.9.0\n","[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n","Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n","Inputs:\n"," tensor([[ 7454, 38746,  7637,  8851],\n","        [ 8918,   465, 13239,   329],\n","        [  262,  1842,   286,   536],\n","        [39236,  1872,    11,   484],\n","        [36385,   319,   257,  7002],\n","        [ 1290,   422,   262,  1964],\n","        [38520,   286, 50224,   328],\n","        [ 1872,    13,  1119, 10282]])\n","\n","Targets:\n"," tensor([[38746,  7637,  8851,  8918],\n","        [  465, 13239,   329,   262],\n","        [ 1842,   286,   536, 39236],\n","        [ 1872,    11,   484, 36385],\n","        [  319,   257,  7002,  1290],\n","        [  422,   262,  1964, 38520],\n","        [  286, 50224,   328,  1872],\n","        [   13,  1119, 10282,   287]])\n","tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","Input batch:\n"," tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","\n","Output shape: torch.Size([2, 4, 50257])\n","tensor([[[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n","           2.1921e-01, -5.8153e-01],\n","         [ 3.7736e-01, -4.2545e-01, -6.5874e-01,  ..., -2.5050e-01,\n","           4.6553e-01, -2.5760e-01],\n","         [ 8.8996e-01, -1.3770e-01,  1.4748e-01,  ...,  1.7770e-01,\n","          -1.2015e-01, -1.8902e-01],\n","         [-9.7276e-01,  9.7338e-02, -2.5419e-01,  ...,  1.1035e+00,\n","           3.7639e-01, -5.9006e-01]],\n","\n","        [[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n","           2.1921e-01, -5.8153e-01],\n","         [ 1.3433e-01, -2.1289e-01, -2.7021e-02,  ...,  8.1153e-01,\n","          -4.7410e-02,  3.1186e-01],\n","         [ 8.9996e-01,  9.5396e-01, -1.7896e-01,  ...,  8.3053e-01,\n","           2.7657e-01, -2.4577e-02],\n","         [-9.3430e-05,  1.9390e-01,  5.1217e-01,  ...,  1.1915e+00,\n","          -1.6431e-01,  3.7046e-02]]], grad_fn=<UnsafeViewBackward0>)\n","encoded: [40630, 3804, 11, 290, 14923]\n","encoded_tensor.shape: torch.Size([1, 5])\n","Output: tensor([[40630,  3804,    11,   290, 14923, 14487, 31458, 24810,  3055,  9183,\n","         32327]])\n","Output length: 11\n","Years passed, and rumors Combat Referenceslastingechprofit delightful\n","matplotlib version: 3.10.0\n","numpy version: 1.26.4\n","tiktoken version: 0.9.0\n","torch version: 2.5.1+cu124\n","Output text:\n"," Once Aadhan renounced his kingdom Criticsatu frig festivals highlightingDa stren Mortgage TTAdvertisement\n","Once Aadhan renounced his kingdom for the love of Thamarai, they embarked on a journey far from the\n","e of strength and inspiration, guiding them through the challenges and adventures that lay ahead.\n","\n","\n","Characters: 9728\n","Tokens: 1952\n","Train loader:\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","\n","Validation loader:\n","Training tokens: 1536\n","Validation tokens: 0\n","All tokens: 1536\n","Training loss: 10.982452710469564\n","Validation loss: nan\n","Ep 1 (Step 000000): Train loss 9.138, Val loss nan\n","Once Aadhan renounced his kingdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n","Ep 2 (Step 000005): Train loss 6.831, Val loss nan\n","Once Aadhan renounced his kingdom, and the, and the,, and the, and the, and the, and, and the, and the, and the, and the, and the, and the, and the, and the, and, and the, and the\n","Once Aadhan renounced his kingdom, Aadhan and Maria, Aadhan and Maria.                                       \n","Ep 4 (Step 000010): Train loss 4.726, Val loss nan\n","Once Aadhan renounced his kingdom, Aadhan and Maria, and Maria, and Maria.                                      \n","Once Aadhan renounced his kingdom, Aadhan'shan'sai, theyhan's the beauty and Maria, Aadhan'shan'shan's love and Maria.                       \n","Ep 6 (Step 000015): Train loss 3.214, Val loss nan\n","Once Aadhan renounced his kingdom, Aadhan'shan'sai, they had.         As Aadhan's the and Maria the beauty of the beauty of Kandigai, they had to the the, and Maria and Maria.  \n","Ep 7 (Step 000020): Train loss 2.166, Val loss nan\n","Once Aadhan renounced his kingdom for the the peoplehan's love, they had to the and compassion of Kandigai by his brother's love and Thamarai by his brother's.                  \n","Once Aadhan renounced his kingdom for the the.                                              \n","Ep 9 (Step 000025): Train loss 1.376, Val loss nan\n","Once Aadhan renounced his kingdom for the the land now lay barren, Aadhan and Thamarai forged a powerful partnership, he had to the love blossomed amidst the beauty of the.                  \n","Once Aadhan renounced his kingdom for the love of Thamarai, its people living in fear and oppression. Aadhan knew he had to act swiftly to overthrow his brother's oppressive regime.                  \n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 500x300 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Output text:\n"," Once Aadhan renounced his kingdom for the love of Thamarai, they embarked\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-e8db75afc06a>:918: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'GPT_CONFIG' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e8db75afc06a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;31m# Define context size based on model configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m \u001b[0mcontext_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;31m# Generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GPT_CONFIG' is not defined"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Story_LLM.ipynb\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1f27dtc5Luh_DNONJIKTD9jsezgjhhN0s\n","\n","**LLM Workshop 2024 by Sebastian Raschka**\n","\n","This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)\n","\"\"\"\n","\n","\n","\n","\"\"\"<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2) Understanding LLM Input Data\n","\"\"\"\n","\n","#! pip install torch >= 2.0.1\n","#! pip install jupyterlab >= 4.0\n","!pip install tiktoken #>= 0.5.1\n","!pip install matplotlib #>= 3.7.1\n","#! numpy >= 1.24.3\n","#! tensorflow >= 2.15.0\n","#! tqdm >= 4.66.1\n","#! numpy >= 1.25, < 2.0\n","#! pandas >= 2.2.1\n","#! psutil >= 5.9.5\n","!pip install litgpt[all] #>= 0.4.1\n","\n","\"\"\"Packages that are being used in this notebook:\"\"\"\n","\n","#---------------------------------------------\n","\n","#Below is the code for supplementary\n","\n","#---------------------------------------------\n","\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","import tiktoken\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n","\n","    return dataloader\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec)  # optional projection\n","\n","        return context_vec\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches\n","\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n","    return encoded_tensor\n","\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0)  # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","        decoded_text = token_ids_to_text(token_ids, tokenizer)\n","        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","\n","\n","def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n","    fig, ax1 = plt.subplots(figsize=(5, 3))\n","\n","    # Plot training and validation loss against epochs\n","    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n","    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"Loss\")\n","    ax1.legend(loc=\"upper right\")\n","    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n","\n","    # Create a second x-axis for tokens seen\n","    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n","    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n","    ax2.set_xlabel(\"Tokens seen\")\n","\n","    fig.tight_layout()  # Adjust layout to make room\n","    plt.savefig(\"loss-plot.pdf\")\n","    plt.show()\n","\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","\n","from importlib.metadata import version\n","\n","\n","print(\"torch version:\", version(\"torch\"))\n","print(\"tiktoken version:\", version(\"tiktoken\"))\n","\n","\"\"\"- This notebook provides a brief overview of the data preparation and sampling procedures to get input data \"ready\" for an LLM\n","- Understanding what the input data looks like is a great first step towards understanding how LLMs work\n","\n","<img src=\"./figures/01.png\" width=\"1000px\">\n","\n","<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.1 Tokenizing text\n","\n","- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters\n","\n","<img src=\"figures/02.png\" width=\"800px\">\n","\n","- Load raw text we want to work with\n","- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story\n","\"\"\"\n","\n","#with open(\"/content/sample_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","with open(\"/content/sample_data/Short_Story.txt\", \"r\", encoding=\"utf-8\") as f:\n","\n","    raw_text = f.read()\n","\n","print(\"Total number of character:\", len(raw_text))\n","print(raw_text[:99])\n","\n","\"\"\"- The goal is to tokenize and embed this text for an LLM\n","- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n","\n","<img src=\"figures/03.png\" width=\"690px\">\n","\n","- The following regular expression will split on whitespaces and punctuation\n","\"\"\"\n","\n","import re\n","\n","preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item for item in preprocessed if item]\n","print(preprocessed[:38])\n","\n","print(\"Number of tokens:\", len(preprocessed))\n","\n","\"\"\"<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.2 Converting tokens into token IDs\n","\n","- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n","- For this we first need to build a vocabulary\n","\n","<img src=\"figures/04.png\" width=\"900px\">\n","\n","- The vocabulary contains the unique words in the input text\n","\"\"\"\n","\n","all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","\n","print(vocab_size)\n","\n","vocab = {token:integer for integer,token in enumerate(all_words)}\n","\n","\"\"\"- Below are the first 50 entries in this vocabulary:\"\"\"\n","\n","for i, item in enumerate(vocab.items()):\n","    print(item)\n","    if i >= 50:\n","        break\n","\n","\"\"\"- Below, we illustrate the tokenization of a short sample text using a small vocabulary:\n","\n","<img src=\"figures/05.png\" width=\"800px\">\n","\n","- Let's now put it all together into a tokenizer class\n","\"\"\"\n","\n","class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text\n","\n","\"\"\"- The `encode` function turns text into token IDs\n","- The `decode` function turns token IDs back into text\n","\n","<img src=\"figures/06.png\" width=\"800px\">\n","\n","- We can use the tokenizer to encode (that is, tokenize) texts into integers\n","- These integers can then be embedded (later) as input of/for the LLM\n","\"\"\"\n","\n","tokenizer = SimpleTokenizerV1(vocab)\n","\n","#text = \"\"\"\"It's the last he painted, you know,\"\n","#           Mrs. Gisburn said with pardonable pride.\"\"\"\n","\n","text = \"\"\"With Thamarai by his side, Aadhan journeyed back to his homeland.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)\n","\n","\"\"\"- We can decode the integers back into text\"\"\"\n","\n","tokenizer.decode(ids)\n","\n","tokenizer.decode(tokenizer.encode(text))\n","\n","\"\"\"<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.3 BytePair encoding\n","\n","- GPT-2 used BytePair encoding (BPE) as its tokenizer\n","- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n","- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n","- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n","- In this lecture, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n","- (Based on an analysis [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb), I found that `tiktoken` is approx. 3x faster than the original tokenizer and 6x faster than an equivalent tokenizer in Hugging Face)\n","\"\"\"\n","\n","# pip install tiktoken\n","\n","import importlib\n","import tiktoken\n","\n","print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(integers)\n","\n","strings = tokenizer.decode(integers)\n","\n","print(strings)\n","\n","\"\"\"- BPE tokenizers break down unknown words into subwords and individual characters:\n","\n","<img src=\"figures/07.png\" width=\"700px\">\n","\"\"\"\n","\n","tokenizer.encode(\"Akwirw ier\", allowed_special={\"<|endoftext|>\"})\n","\n","\"\"\"<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.4 Data sampling with a sliding window\n","\n","- Above, we took care of the tokenization (converting text into word tokens represented as token ID numbers)\n","- Now, let's talk about how we create the data loading for LLMs\n","- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict\n","\n","<img src=\"figures/08.png\" width=\"800px\">\n","\n","- For this, we use a sliding window approach, changing the position by +1:\n","\n","<img src=\"figures/09.png\" width=\"900px\">\n","\n","- Note that in practice it's best to set the stride equal to the context length so that we don't have overlaps between the inputs (the targets are still shifted by +1 always)\n","\"\"\"\n","\n","from google.colab import files\n","#src = list(files.upload().values())[0]\n","#src = read_text_file(\"supplementary.py\")\n","#src = open(r\"D:\\Text\\MyFile2.txt\",\"w+\")\n","\n","#open('/content/sample_data/supplementary.py','wb').write(src)\n","\n","\n","\n","\"\"\"<img src=\"figures/10.png\" width=\"800px\">\"\"\"\n","\n","#from supplementary import create_dataloader_v1\n","\n","\n","dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"\\nTargets:\\n\", targets)\n","\n","\"\"\"<br>\n","<br>\n","<br>\n","<br>\n","\n","# Exercise: Prepare your own favorite text dataset\n","\"\"\"\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.0,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","\n","import torch.nn as nn\n","#from supplementary import TransformerBlock, LayerNorm\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","import torch\n","import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","batch = []\n","\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","\n","out = model(batch)\n","print(\"Input batch:\\n\", batch)\n","print(\"\\nOutput shape:\", out.shape)\n","print(out)\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","model.eval();  # disable dropout\n","\n","start_context = \"Years passed, and rumors\"\n","\n","encoded = tokenizer.encode(start_context)\n","print(\"encoded:\", encoded)\n","\n","encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n","\n","out = generate_text_simple(\n","    model=model,\n","    idx=encoded_tensor,\n","    max_new_tokens=6,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output:\", out)\n","print(\"Output length:\", len(out[0]))\n","\n","decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","print(decoded_text)\n","\n","from importlib.metadata import version\n","\n","pkgs = [\"matplotlib\",\n","        \"numpy\",\n","        \"tiktoken\",\n","        \"torch\",\n","       ]\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")\n","\n","import torch\n","#from supplementary import GPTModel\n","\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference\n","\n","import tiktoken\n","#from supplementary import generate_text_simple\n","\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Once Aadhan renounced his kingdom\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n","\n","with open(\"/content/sample_data/Short_Story.txt\", \"r\", encoding=\"utf-8\") as file:\n","    text_data = file.read()\n","\n","# First 100 characters\n","print(text_data[:99])\n","\n","# Last 100 characters\n","print(text_data[-99:])\n","\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)\n","\n","#from supplementary import create_dataloader_v1\n","\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)\n","\n","train_tokens = 0\n","for input_batch, target_batch in train_loader:\n","    train_tokens += input_batch.numel()\n","\n","val_tokens = 0\n","for input_batch, target_batch in val_loader:\n","    val_tokens += input_batch.numel()\n","\n","print(\"Training tokens:\", train_tokens)\n","print(\"Validation tokens:\", val_tokens)\n","print(\"All tokens:\", train_tokens + val_tokens)\n","\n","#from supplementary import calc_loss_loader\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n","\n","\n","torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n","\n","with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device)\n","    val_loss = calc_loss_loader(val_loader, model, device)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)\n","\n","#4.3 Training an LLM\n","\n","#from supplementary import (\n","#    calc_loss_batch,\n","#    evaluate_model,\n","#    generate_and_print_sample\n","#)\n","\n","\n","def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Once Aadhan renounced his kingdom\", tokenizer=tokenizer\n",")\n","\n","\n","\n","torch.save(model.state_dict(), \"model.pth\")\n","\n","#from supplementary import plot_losses\n","\n","\n","epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n","plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n","\n","#Solution to Exercise 1\n","\n","start_context = \"Once Aadhan renounced his kingdom\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer).to(device),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n","\n","device\n","\n","import torch\n","\n","# Imports from a local file\n","f#rom supplementary import GPTModel\n","\n","\n","model = GPTModel(GPT_CONFIG_124M)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n","model.eval();\n","\n","start_text = \"Gathering allies among the disgruntled citizens\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# Convert text to token IDs\n","encoded_text = torch.tensor(tokenizer.encode(start_text)).unsqueeze(0)  # Add batch dimension\n","\n","# Define context size based on model configuration\n","context_size = GPT_CONFIG_124M[\"context_length\"]\n","\n","# Generate text\n","generated_tokens = generate_text_simple(model, encoded_text, max_new_tokens=10, context_size=context_size)\n","\n","# Decode and print generated text\n","print(\"Generated Text:\", tokenizer.decode(generated_tokens.squeeze(0).tolist()))\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"05SAzeu-yHS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","start_text = \"Gathering allies among the disgruntled citizens\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# Convert text to token IDs\n","encoded_text = torch.tensor(tokenizer.encode(start_text)).unsqueeze(0)  # Add batch dimension\n","\n","# Define context size based on model configuration\n","context_size = GPT_CONFIG_124M[\"context_length\"]\n","\n","# Generate text\n","generated_tokens = generate_text_simple(model, encoded_text, max_new_tokens=10, context_size=context_size)\n","\n","# Decode and print generated text\n","print(\"Generated Text:\", tokenizer.decode(generated_tokens.squeeze(0).tolist()))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFkQJIl6Rl0h","executionInfo":{"status":"ok","timestamp":1741328642178,"user_tz":-330,"elapsed":1467,"user":{"displayName":"Rajesh Learning","userId":"02323984841585441667"}},"outputId":"dd72470e-10ef-4e46-f6bc-563167bd8806"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: Gathering allies among the disgruntled citizens, from the golden beaches of the Algarve\n"]}]}]}